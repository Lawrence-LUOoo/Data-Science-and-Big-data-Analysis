data.test <- data.sample[n_training:n,]
data.training
data.test <- data.sample[n_training:n,]
data.test
# fit linear regression
model <- lm(Y~X,data=data.training)
plot(Y~X,data=data.training)
abline(model)
# The quality of the model can be estimated by the MSE
# mean squared error of the prediction. This can
# be found by finding the RSS (sum of squared residuals)
# and dividing by n or mean squared residual.
# quickway: MSE_model <- mean(resid(model)^2)
# alternatively:
training_predict <- predict(model)
summary(model)
screenreg(model)
# LOOCV validation example
set.seed(101)
# Let's create a population with some
# variable of interest Y. We have an
# input variable X that has mean 800, stdev 10
# and 10000 members
N <- 10000
X <- rnorm(N,mean=800,sd=10)
# Let's make the true relationship that
# Y and X are positively correlated.
# with population regression relationship
# Y = 0.5*X + e
# where e is the variance in Y that is not explained
# by the variation in X
# assume this is fairly large
# e.g. mean = 0 but sd = 20
e <- rnorm(N,mean=0,sd=20)
# now we can calculate Y
Y = 0.5*X + e
# Store into data.frame
data.pop = data.frame(Y,X,e)
# Suppose a data scientist wants to explore the
# relationship between Y and X. However they don't have
# access to the full population sample, and need
# to infer it from a random sample of 40 points
n=40
sampled_rows <- sample(1:N,n )
data.sample <- data.pop[sampled_rows,]
# fit linear regression
model <- lm(Y~X,data=data.sample)
# create diagnostic plots for fit
# plot(model)
plot(Y~X,data=data.sample)
abline(model)
summary(model)
data.sample$cv_pred
# LOOCV validation example
set.seed(101)
# Let's create a population with some
# variable of interest Y. We have an
# input variable X that has mean 800, stdev 10
# and 10000 members
N <- 10000
X <- rnorm(N,mean=800,sd=10)
# Let's make the true relationship that
# Y and X are positively correlated.
# with population regression relationship
# Y = 0.5*X + e
# where e is the variance in Y that is not explained
# by the variation in X
# assume this is fairly large
# e.g. mean = 0 but sd = 20
e <- rnorm(N,mean=0,sd=20)
# now we can calculate Y
Y = 0.5*X + e
# Store into data.frame
data.pop = data.frame(Y,X,e)
# Suppose a data scientist wants to explore the
# relationship between Y and X. However they don't have
# access to the full population sample, and need
# to infer it from a random sample of 40 points
n=40
sampled_rows <- sample(1:N,n )
data.sample <- data.pop[sampled_rows,]
# fit linear regression
model <- lm(Y~X,data=data.sample)
# create diagnostic plots for fit
# plot(model)
plot(Y~X,data=data.sample)
abline(model)
summary(model)
data.sample
data.sample$cv_pred <- NA
data.sample
nrows(data.sample)
nrow(data.sample)
# bootstrap validation example
set.seed(101)
# Let's create a population with some
# variable of interest Y. We have an
# input variable X that has mean 800, stdev 10
# and 10000 members
N <- 10000
X <- rnorm(N,mean=800,sd=10)
# Let's make the true relationship that
# Y and X are positively correlated.
# with population regression relationship
# Y = 0.5*X + e
# where e is the variance in Y that is not explained
# by the variation in X
# assume this is fairly large
# e.g. mean = 0 but sd = 20
e <- rnorm(N,mean=0,sd=20)
# now we can calculate Y
Y = 0.5*X + e
# Store into data.frame
data.pop = data.frame(Y,X,e)
# Suppose a data scientist wants to explore the
# relationship between Y and X. However they don't have
# access to the full population sample, and need
# to infer it from a random sample of 200 points
n <- 200
sampled_rows <- sample(1:N,n )
data.sample <- data.pop[sampled_rows,]
# in bootstrapping we resample this many times
R=1000
# on each resampled set we calculate and store
# the fit parameter of interest
# here let's store the fitted slope value
# and mean squared sum of residuals MSE
coef_out <- rep(NA,R)
MSE_out <- rep(NA,R)
for (i in 1:R){
resampled_rows <- sample(1:n,n,replace=TRUE )
data.resampled <- data.sample[resampled_rows,]
model <- lm(Y~X,data=data.resampled)
coef_out[i] <- coef(model)[[2]]
MSE_out[i] <- mean(resid(model)^2)
}
# examining the histograms of the variables of interest
# let's us estimate the possible distribution
# when testing new data
hist(coef_out)
hist(MSE_out)
coef_out
model
coef(model)
coef(model)[[2]]
library(readr)
library(MASS)
library(dplyr)
library(tidyr)
# PREPARE DATASET
#read in dataset
house_data_raw <- read_csv("train.csv")
# create new columns logSalePrice
house_data$logSalePrice <- log10(house_data$SalePrice)
# select columns that match is.numeric
house_data_num<- select_if(house_data, is.numeric)
# MASS Library also includes a select function
# to ensure we use dplyr select use :: to
# specify we want the dplyr version
house_data_num<- dplyr::select(house_data_num, -SalePrice)
# remove rows with missing data
house_data_num <- drop_na(house_data_num)
# PERFORM STEPWISE MODEL SELECTION
# we first  define the minimum and maximum model sizes
lm.min <- s
lm.max <- lm(logSalePrice ~ ., data=house_data_num)
# and put them in a list that can be used as an argument
# when calling the stepAIC function
scp <- list(lower = lm.min, upper = lm.max)
lm.selected <- stepAIC(lm.min,
direction = 'forward',
scope = scp,
steps = 4)
library(readr)
library(MASS)
library(dplyr)
library(tidyr)
# PREPARE DATASET
#read in dataset
house_data_raw <- read_csv("train.csv")
# create new columns logSalePrice
house_data$logSalePrice <- log10(house_data$SalePrice)
# select columns that match is.numeric
house_data_num<- select_if(house_data, is.numeric)
setwd("~/Desktop/Data Science Summer School/exam/Day 6/Ames House Price data")
library(readr)
library(MASS)
library(dplyr)
library(tidyr)
# PREPARE DATASET
#read in dataset
house_data_raw <- read_csv("train.csv")
# create new columns logSalePrice
house_data$logSalePrice <- log10(house_data$SalePrice)
# select columns that match is.numeric
house_data_num<- select_if(house_data, is.numeric)
# MASS Library also includes a select function
# to ensure we use dplyr select use :: to
# specify we want the dplyr version
house_data_num<- dplyr::select(house_data_num, -SalePrice)
# remove rows with missing data
house_data_num <- drop_na(house_data_num)
# PERFORM STEPWISE MODEL SELECTION
# we first  define the minimum and maximum model sizes
lm.min <- s
lm.max <- lm(logSalePrice ~ ., data=house_data_num)
# and put them in a list that can be used as an argument
# when calling the stepAIC function
scp <- list(lower = lm.min, upper = lm.max)
lm.selected <- stepAIC(lm.min,
direction = 'forward',
scope = scp,
steps = 4)
# PREPARE DATASET
#read in dataset
house_data_raw <- read_csv("train.csv")
# create new columns logSalePrice
house_data$logSalePrice <- log10(house_data$SalePrice)
# select columns that match is.numeric
house_data_num<- select_if(house_data, is.numeric)
# create new columns logSalePrice
house_data$logSalePrice <- log10(house_data_raw$SalePrice)
# PREPARE DATASET
#read in dataset
house_data_raw <- read_csv("train.csv")
# create new columns logSalePrice
house_data$logSalePrice <- log10(house_data_raw$SalePrice)
log10(house_data_raw$SalePrice)
# PREPARE DATASET
#read in dataset
house_data <- read_csv("train.csv")
house_data$SalePrice
# create new columns logSalePrice
house_data$logSalePrice <- log10(house_data$SalePrice)
# select columns that match is.numeric
house_data_num<- select_if(house_data, is.numeric)
# MASS Library also includes a select function
# to ensure we use dplyr select use :: to
# specify we want the dplyr version
house_data_num<- dplyr::select(house_data_num, -SalePrice)
# remove rows with missing data
house_data_num <- drop_na(house_data_num)
lm.min <- s
lm.max <- lm(logSalePrice ~ ., data=house_data_num)
# we first  define the minimum and maximum model sizes
lm.min <- lm(logSalePrice ~ 1, data=house_data_num)
lm.max <- lm(logSalePrice ~ ., data=house_data_num)
scp <- list(lower = lm.min, upper = lm.max)
lm.selected <- stepAIC(lm.min,
direction = 'forward',
scope = scp,
steps = 4)
summary(lm.selected)
plot(house_data$logSalePrice~lm.selected$fitted.values)
lm.selected$fitted.values
house_data$logSalePrice
plot(house_data$logSalePrice~lm.selected$fitted.values)
plot(lm.selected$residuals~lm.selected$fitted.values)
nrow(house_data$logSalePrice)
house_data
house_data$logSalePrice
house_data$logSalePrice
plot(house_data$logSalePrice~lm.selected$fitted.values)
house_data$logSalePrice
nrow(house_data$logSalePrice)
length(house_data$logSalePrice)
length(lm.selected$fitted.values)
library(gvlma)
gvlma(lm.selected)
gvlma(lm.selected)
library(car)
vif(lm.selected)
housing_data <- read_csv("train.csv")
housing_data$logSalePrice <- log10(housing_data$SalePrice)
housing_data <- select_if(housing_data, is.numeric)
set.seed(1)
n <- nrow(housing_data)
random_row_ids <- sample(1:n,size=n,replace=TRUE)
housing_data_rs <- housing_data[random_row_ids,]
mean(housing_data$logSalePrice)
mean(housing_data_rs$logSalePrice)
plot(logSalePrice~GrLivArea,data=housing_data, pch=1)
points(logSalePrice~GrLivArea,data=housing_data_rs, col="red", pch=2)
library(dplyr)
library(readr)
library(dplyr)
library(tidyr)
housing_data <- read_csv("train.csv")
housing_data$logSalePrice <- log10(housing_data$SalePrice)
housing_data <- select_if(housing_data, is.numeric)
# glmnet does not like NA values...
# either remove problematic rows/columns
# in this case e.g. MiscFeature=NA means number of misc features = 0
# so we replace the NAs with 0 using the following code:
housing_data[is.na(housing_data)] <- 0
library(glmnet)
housing_data.x <- dplyr::select(housing_data,  -SalePrice, -logSalePrice)
housing_data.x <- data.matrix(housing_data.x)
housing_data.y <- housing_data$logSalePrice
# glmnet can do ridge regression alpha=0
#            or lasso regression alpha=1
# we can also use values between 0 and 1
# which uses a hybrid of the two penalty types
# (called elastic net)
fit_lasso <- glmnet(housing_data.x, housing_data.y,
alpha = 1, lambda = 1)
fit_ridge <- glmnet(housing_data.x, housing_data.y,
alpha = 0, lambda = 1)
# view fit coefficients
coef(fit_lasso)
# to predict new points based on model
pred <- predict(fit_lasso,newx=housing_data.x)
# if we do not specify lambda glmnet will test out
# a range of penalties and we can view how coefficents
# behave by plotting the result
fit_lasso <- glmnet(housing_data.x, housing_data.y,
alpha = 1)
plot(fit_lasso, xvar = "lambda") # lambda is on log-scale
# cross validation for best choice of lambda
results <- cv.glmnet(housing_data.x, housing_data.y,
alpha = 0, nfolds=10)
plot(results)
setwd("~/Desktop/Data Science Summer School/exam/Day 8")
# Open file 'mf_training.Rda'
load("mf_training.Rda")
# Open file 'mf_training.Rda'
load("mf_training.Rda")
# Examine the data, and make exploratory plots to test variables
# that might be good predictors for height
# Comment on your findings.
par(mfrow=c(2,2))
# the following command creates a 2x2 plot but
# adjusts margins (plot spacing) to be smaller
par(mfcol=c(2,2), mar=c(4,4,0.5,0.5), oma=c(1.5,2,1,1))
# the following command creates a 2x2 plot but
# adjusts margins (plot spacing) to be smaller
par(mfcol=c(2,2), mar=c(4,4,0.5,0.5), oma=c(1.5,2,1,1))
# with command lets you run a
# command using a given data set
# similar to using the data=... argument
with(mf_training,  scatter.smooth(height ~ hair,
pch=20,
col = ifelse(gender==1,'red','blue')
)
)
plot(height ~ eye, data=mf_training)
plot(height ~ as.factor(gender), data=mf_training)
plot(height ~ as.factor(glasses), data=mf_training)
model = lm(height ~ hair + gender + glasses + eye, data=mf_training)
summary(model)
mse<-mean(residuals(model)^2)
rmse<-sqrt(mse)
# mse 43.8 cm^2
# rmse 6.62 cm
library(leaps)
regsubsets.out <-
regsubsets(height ~ ., # full model
data = mf_training,	# dataset
nbest = 1,       # 1 best model for each number of predictors
nvmax = NULL,    # NULL for no limit on number of variables
force.in = NULL, force.out = NULL, # edit if you want to force variables in / out
method = "exhaustive") # tries every combination
regsubsets.out
summary.out <- summary(regsubsets.out)
as.data.frame(summary.out$outmat)
as.data.frame(summary.out$outmat)
## Adjusted R2 vs number of input variables
par(mfrow=c(1,2))
plot(regsubsets.out, scale="adjr2")
adj_rsquare_values <- summary(regsubsets.out)$adjr2
summary(regsubsets.out)$adjr2
summary.out
summary(regsubsets.out)$adjr2
par(mfrow=c(1,2))
plot(regsubsets.out, scale="adjr2")
adj_rsquare_values <- summary(regsubsets.out)$adjr2
n_predictor_vals <- 1:5
plot(adj_rsquare_values~n_predictor_vals)
# Load and view the dataset
load("mf_training.Rda")
head(mf_training)
which(mf_training$gender==1)
rows_glasses = which( mf_training$glasses==1)
rows_glasses
mf_training[rows_glasses,]
mf_training[rows_glasses,]$gender
mean(mf_training[rows_glasses,]$gender)
# Load and view the dataset
load("mf_training.Rda")
head(mf_training)
nfemales <- length(which(mf_training$gender==1))
nmales <- length(which(mf_training$gender==0))
result <- mean(mf_training$gender)
n_f_glasses <- length(which(mf_training$gender==1 & mf_training$glasses==1))
n_m_glasses <- length(which(mf_training$gender==0 & mf_training$glasses==1))
prob_f_glasses  <- n_f_glasses / (n_f_glasses + n_m_glasses)
prob_f_glasses
#    ii) P(female|no-glasses)
#        probability person from dataset if female if they do not wear glasses
n_f_noglasses <- length(which(mf_training$gender==1 & mf_training$glasses==0))
n_m_noglasses <- length(which(mf_training$gender==0 & mf_training$glasses==0))
prob_f_noglasses  <- n_f_noglasses / (n_f_noglasses + n_m_noglasses)
prob_f_noglasses
rows_glasses = which( mf_training$glasses==1)
result <- mean(mf_training[rows_glasses,]$gender)
result
# Load in the mf_training data
load("mf_training.Rda")
head(mf_training)
gender_height.lm = lm(gender ~ height,mf_training)
plot(mf_training$gender ~ mf_training$height)
abline(gender_height.lm)
summary(gender_height.lm)
# 3) What does the model predict for
# P(female|height=150)
# P(female|height=180)
# Hint. we use the predict function like this to find
# prediction for heights h1, h2 etc
# predict(gender_height.lm, data.frame(height = c(h1, h2 ... )))
predict(gender_height.lm, data.frame(height = c(150, 180)))
# Load in the mf_training data
load("mf_training.Rda")
head(mf_training)
gender_height.lm = lm(gender ~ height,mf_training)
plot(mf_training$gender ~ mf_training$height)
abline(gender_height.lm)
summary(gender_height.lm)
mf_training$gender
predict(gender_height.lm)
# Load in the mf_training data
load("mf_training.Rda")
head(mf_training)
# Note to view discrimination can plot histograms
hist(subset(mf_training,gender==1)$height, col='blue',
xlab='height',main='Male/Females height distributions')
hist(subset(mf_training,gender==0)$height, col=rgb(1,0,0,0.5), add=T)
subset(mf_training,gender==1)
# 1) Now use logistic regression to predict the mean
# value of gender regressed against height
# can you plot the resulting fit?
gender_height.glm=glm(gender~height,data=mf_training,family=binomial)
# to plot this is not so easy... quick google
# In general the command we need to use is:
#<model>=glm(<output>~<input_var>,data=mf_training,family=binomial)
#curve(predict(<model>,data.frame(<input_var>=x),type="resp"),add=TRUE)
# replace names in < > with your own variable names
plot(gender~height,data=mf_training)
curve(predict(gender_height.glm,newdata = height,type="response"),add=TRUE)
curve(predict(gender_height.glm,data.frame(height=x),type="resp"),add=TRUE)
coef(gender_height.glm)
# Load in the mf_training data
load("mf_training.Rda")
head(mf_training)
# More normally we would apply a logistic regression
# this fits a logistic curve that can olny
# take values between 0 and 1 (i.e. can't result in
# impossible probabilities as the linear regression can)
# glm.fit=glm(gender~height,data=mf_training,family=binomial)
# Note to view discrimination can plot histograms
hist(subset(mf_training,gender==1)$height, col='blue',
xlab='height',main='Male/Females height distributions')
hist(subset(mf_training,gender==0)$height, col=rgb(1,0,0,0.5), add=T)
# 1) Now use logistic regression to predict the mean
# value of gender regressed against height
# can you plot the resulting fit?
gender_height.glm=glm(gender~height,data=mf_training,family=binomial)
# to plot this is not so easy... quick google
# In general the command we need to use is:
#<model>=glm(<output>~<input_var>,data=mf_training,family=binomial)
#curve(predict(<model>,data.frame(<input_var>=x),type="resp"),add=TRUE)
# replace names in < > with your own variable names
plot(gender~height,data=mf_training)
curve(predict(gender_height.glm,data.frame(height=x),type="resp"),add=TRUE)
coef(gender_height.glm)
# 2) What is the formula for the fit? This predicts the probability
# that a subject drawn from the dataset and found to have a
# given height, is female
# for logistic fit formula is 1 / (1 + exp(-B0 - B1*X))
# here B0 = 21.07, B1 = -0.123
# so P female =   1/(1 + exp(-21.07 + 0.123*height) )
# note can also add curve using this
# curve(1/(1 + exp(-21.07 + 0.123*x)),col='green',add=TRUE)
# 3) What does the logistic regression model predict for
# P(female|height=150)
# P(female|height=180)
# Hint. we use the predict function like this to find
# prediction for heights h1, h2 etc
# note need additional argument type="resp" for this fit method
predict(gender_height.glm, data.frame(height = c(150, 180)),type="resp")
# 4) At which height H is P(female|height=H) = 0.5
# hint. you can get fit parameters b0 and b1
# using code like:
# b0 = coef(model)[["(Intercept)"]])
# b1 = coef(model)[["height"]]
# and solve for y = 1  / (1  + exp(-b0 - b1*x))
# by inspection when exp(-b0 - b1*x) = 1
# y= 0.5
# so exp(-b0 + -b1*x) = 1
# means - b0 - b1*x = 0
# so crosses at x = -b0/b1 = 21.07 / 0.123 = 171.3 cm
# looks reasonable from the fitted curve
# 5) To make a prediction male or female at height x
# look at the prediction from the regression
# which calculates P(female|x)
#
# If we estimate P(female|x) > 0.5 we predict female (1)
# If we estimate P(female|x) < 0.5 we predict female (0)
# generate columns as we did with the previous value
mf_training$mod1_value <- predict(gender_height.glm,type='resp')
mf_training$mod1_prediction <- 0
mf_training$mod1_prediction <- ifelse(mf_training$mod1_value > 0.5, 1, 0 )
mf_training$mod1_prediction
